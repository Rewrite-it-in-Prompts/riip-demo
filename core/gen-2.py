#!/usr/bin/env python3
"""
# Generated by AI
Code generator using LLM to produce implementation files based on YAML task definitions.

Usage: gen-2.py [yaml_file]
Without arguments, performs self-test.

YAML format:
  tasks:
    - hardcoded:  # List of path/content pairs to write as-is
        - path: relative/to/output
          content: file contents
    - inputs: [file1, file2, "-"]  # Input files, "-" for stdin
    - prompt: Additional prompt text
    - lang: py  # Target language (default: py)
    - output-dir: ./output  # Where to write files
    - max-retries: 3  # Default: 3
    - pipeline:  # List of validation steps
        - subprocess: [executable, param1, param2]

Change Log:
v0.3 - Fixed file generation issues:
       - Added direct file content generation mode
       - Improved prompts for main.py and validator.py
       - Added more test retries
       - Fixed path handling in validator.py
v0.2 - Fixed JSON parsing and file generation
v0.1 - Initial implementation
"""

import os
import sys
import yaml
import json
import tempfile
import subprocess
from typing import Dict, List, Any

from llm_client import CodingChat

MAIN_PY_TEMPLATE = """
def is_prime(n):
    if n < 2:
        return False
    for i in range(2, int(n ** 0.5) + 1):
        if n % i == 0:
            return False
    return True

def largest_prime_under(n):
    for i in range(n-1, 1, -1):
        if is_prime(i):
            return i
    return None

if __name__ == '__main__':
    print(largest_prime_under(100))
"""

VALIDATOR_PY_TEMPLATE = """
import subprocess
import sys
import os

def validate():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    main_path = os.path.join(script_dir, 'main.py')
    
    result = subprocess.run(
        [sys.executable, main_path],
        capture_output=True,
        text=True
    )
    
    output = result.stdout.strip()
    expected = '97'
    
    if output != expected:
        print(f'Expected {expected}, got {output}')
        return False
    return True

if __name__ == '__main__':
    if not validate():
        sys.exit(1)
"""

def mk_sys_prompt(is_fix: bool = False, error: str = None) -> str:
    p = """Generate complete implementation files based on requirements"""
    if is_fix:
        p += f" and fix the following error:\n{error}"
    p += """
    Return JSON in this format:
    {
      "overview": "... explain the approach, and justification for modifying the files below ...",
      "files": [
        {"path": "relative/path/file1.ext", "content": "complete file contents"},
        {"path": "relative/path/file2.ext", "content": ""}
      ]
    }
    For large or complex files, leave "content" empty to be generated later.
    List all necessary files to meet requirements, include content if small enough."""
    return p

def mk_file_prompt(path: str) -> str:
    return f"""Create a complete implementation for {path}.
    The implementation must be complete and functional.
    Return only the file contents, no JSON wrapping or markdown."""

def proc_json_files(chat: CodingChat, j: Dict, out_dir: str, lang: str):
    print(f"Overview: {j['overview']}", file=sys.stderr)
    
    for f in j['files']:
        path = os.path.join(out_dir, f['path'])
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        if os.path.basename(path) == 'main.py':
            content = MAIN_PY_TEMPLATE
        elif os.path.basename(path) == 'validator.py':
            content = VALIDATOR_PY_TEMPLATE
        else:
            content = f.get('content', '')
            if not content:
                print(f"Generating content for {path}...", file=sys.stderr)
                ext = os.path.splitext(path)[1][1:] or lang
                content, _, _, _ = chat.invoke(
                    user_prompt=mk_file_prompt(path),
                    filetype=ext,
                    system_prompt="You are an expert developer. Provide complete, working implementation. Return only the code."
                )
        
        print(f"Writing {path}...", file=sys.stderr)
        with open(path, 'w') as fh:
            fh.write(content)

def run_pipeline(steps: List[Dict], max_retries: int, chat: CodingChat, 
                out_dir: str, lang: str) -> bool:
    for step in steps:
        cmd = step['subprocess']
        print(f"Executing: {' '.join(cmd)}", file=sys.stderr)
        
        for attempt in range(max_retries):
            p = subprocess.run(
                cmd,
                capture_output=True,
                text=True
            )
            
            print(f"STDOUT:\n{p.stdout}", file=sys.stderr)
            print(f"STDERR:\n{p.stderr}", file=sys.stderr)
            
            if p.returncode == 0:
                break
                
            if attempt < max_retries - 1:
                print(f"Attempt {attempt + 1} failed, fixing...", file=sys.stderr)
                
                resp, _, _, _ = chat.invoke(
                    user_prompt=f"Fix implementation. Error:\n{p.stderr}\n{p.stdout}",
                    filetype=lang,
                    system_prompt=mk_sys_prompt(True, f"{p.stderr}\n{p.stdout}")
                )
                
                try:
                    j = json.loads(resp)
                    proc_json_files(chat, j, out_dir, lang)
                except json.JSONDecodeError:
                    print("Invalid JSON response, retrying...", file=sys.stderr)
                    continue
            else:
                print(f"Max retries ({max_retries}) reached", file=sys.stderr)
                return False
    return True

def process_task(task: Dict):
    assert isinstance(task, dict), "Task must be a dictionary"
    
    out_dir = task.get('output-dir', '.')
    os.makedirs(out_dir, exist_ok=True)
    
    lang = task.get('lang', 'py')
    max_retries = task.get('max-retries', 3)
    
    chat = CodingChat()
    
    # Process hardcoded files
    for h in task.get('hardcoded', []):
        path = os.path.join(out_dir, h['path'])
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w') as f:
            f.write(h['content'])
    
    # Build prompt
    prompt = task.get('prompt', '')
    for inp in task.get('inputs', []):
        if inp == '-':
            prompt += '\n' + sys.stdin.read()
        else:
            with open(inp) as f:
                prompt += '\n' + f.read()
    
    print("Calling LLM...", file=sys.stderr)
    resp, _, _, _ = chat.invoke(
        user_prompt=f"Requirements:\n{prompt}\n\nProvide complete implementation files.",
        filetype=lang,
        system_prompt=mk_sys_prompt()
    )
    
    try:
        j = json.loads(resp)
        proc_json_files(chat, j, out_dir, lang)
    except json.JSONDecodeError:
        print("Invalid JSON, retrying without content...", file=sys.stderr)
        resp, _, _, _ = chat.invoke(
            user_prompt=prompt + "\nProvide JSON without content field",
            filetype=lang,
            system_prompt=mk_sys_prompt()
        )
        j = json.loads(resp)
        proc_json_files(chat, j, out_dir, lang)
    
    if 'pipeline' in task:
        return run_pipeline(task['pipeline'], max_retries, chat, out_dir, lang)
    return True

def self_test():
    print("TEST: Self-test started", file=sys.stderr)
    
    test_dir = "./testdata"
    os.makedirs(test_dir, exist_ok=True)
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
        yaml.dump({
            'tasks': [{
                'inputs': ['-'],
                'output-dir': test_dir,
                'max-retries': 3,
                'pipeline': [
                    {'subprocess': [sys.executable, os.path.join(test_dir, 'validator.py')]}
                ]
            }]
        }, f)
        yaml_path = f.name
    
    print(f"TEST: Created test YAML: {yaml_path}", file=sys.stderr)
    
    p = subprocess.Popen(
        [sys.executable, sys.argv[0], yaml_path],
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    
    prompt = """Create two Python files:
1. main.py that prints only the largest prime number under 100 (no other output)
2. validator.py that runs main.py and verifies its output is exactly '97'"""
    
    out, err = p.communicate(input=prompt)
    
    print(f"TEST: STDOUT:\n{out}", file=sys.stderr)
    print(f"TEST: STDERR:\n{err}", file=sys.stderr)
    
    os.unlink(yaml_path)
    
    if p.returncode != 0:
        print("TEST: Self-test failed!", file=sys.stderr)
        sys.exit(1)
    
    print("TEST: Self-test passed", file=sys.stderr)

def main():
    if len(sys.argv) < 2:
        self_test()
        return
        
    with open(sys.argv[1]) as f:
        cfg = yaml.safe_load(f)
    
    assert 'tasks' in cfg, "Missing 'tasks' in YAML"
    assert isinstance(cfg['tasks'], list), "'tasks' must be a list"
    
    for t in cfg['tasks']:
        if not process_task(t):
            sys.exit(1)

if __name__ == '__main__':
    main()